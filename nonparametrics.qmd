---
title: "NonParametric & Counting Statistics"
format: html
author: Chris Pleasant
editor: visual
---
 
 
 
```{r}
library(ggplot2)
library(dplyr)
 
# Question 1: 
total_patients <- 19
side_effect_patients <- 3
expected_rate <- 0.5
binom_test <- binom.test(side_effect_patients, total_patients, p = expected_rate, alternative = "two.sided")
print(binom_test)
```
Analysis:
I used a binomial test because we’re comparing how many patients reported side effects (3 out of 19) to an expected rate (50%, or about 9.5 out of 19). This is a simple way to check if the new medication change made a difference.

Results:
The p-value came out to 0.0044, which is super small—way less than 0.05. This means it’s really unlikely that the new rate (3 out of 19) is just random chance.

Interpretation:
So yeah, the medication change actually seems to work. The side effects dropped way below what we’d expect if nothing had changed.

```{r}
# Question 2
# Load NTU data
ntu_data <- read.csv("ntu_data.csv")
#  NTU bins
ntu_data$NTU_bins <- cut(ntu_data$NTU, breaks = c(0, 15, 22, 30, Inf), labels = c("0-15", "16-22", "23-30", "31+"))
# Contingency table
contingency_table <- table(ntu_data$Treatment, ntu_data$NTU_bins)
# Chi-Square Test
chi_square_test <- chisq.test(contingency_table)
print(chi_square_test)

# Turbidity Distribution 
ntu_plot <- ggplot(ntu_data, aes(x = Treatment, fill = NTU_bins)) +
  geom_bar(position = "stack") +
  labs(title = "Turbidity Distribution by Treatment", x = "Treatment", y = "Count") +
  theme_minimal()
print(ntu_plot)
```
Analysis:
Here, I binned the turbidity levels into four categories (0-15, 16-22, 23-30, and 31+ NTU) because that’s what the question asked for. Then, I made a contingency table to see how often each treatment led to turbidity in those categories. A chi-square test checks if the treatments affect turbidity differently.

Results:
The p-value was 0.0006, which is tiny. This tells me there’s a really clear difference between how the two treatments affect turbidity.

Interpretation:
Both treatments don’t do the same thing—one of them probably works better at keeping turbidity low (or one is worse). The bar graph I made also shows that the distribution of turbidity levels isn’t the same for the two treatments.
```{r}
# Question 3
# GradSchool data
grad_school_data <- read.csv("grad_school.csv")
# Spearman Correlation
spearman_corr <- cor.test(grad_school_data$GPA, grad_school_data$GRE, method = "spearman")
print(spearman_corr)

# GPA vs GRE
gpa_gre_plot <- ggplot(grad_school_data, aes(x = GPA, y = GRE)) +
  geom_point() +
  labs(title = "Scatterplot of GPA vs GRE", x = "GPA", y = "GRE Score") +
  theme_minimal()
print(gpa_gre_plot)
```
Analysis:
I ran a Spearman correlation to see if there’s a relationship between GPA and GRE scores. I chose Spearman instead of Pearson because it’s more flexible and doesn’t assume the data is perfectly linear.

Results:
The correlation coefficient was 0.585, and the p-value was 0.046. This means there’s a moderate positive relationship between GPA and GRE scores, and it’s statistically significant (barely!).

Interpretation:
So yeah, students with higher GPAs tend to get better GRE scores, but it’s not a perfect relationship. The scatterplot kind of shows this too—you can see a trend, but it’s not super tight.

```{r}
# Question 4: 
# DogwoodSeeds data
dogwood_seeds_data <- read.csv("DogwoodSeeds.csv")
# Kruskal-Wallis Test
kruskal_test <- kruskal.test(Seeds ~ Treatment, data = dogwood_seeds_data)
print(kruskal_test)

# Dogwood Seed Yield by Treatment
dogwood_plot <- ggplot(dogwood_seeds_data, aes(x = Treatment, y = Seeds)) +
  geom_boxplot() +
  labs(title = "Dogwood Seed Yield by Treatment", x = "Treatment", y = "Number of Seeds") +
  theme_minimal()
print(dogwood_plot)

```
Analysis:
I used the Kruskal-Wallis test here because we’re comparing seed yield across four different treatments, and the data might not be normally distributed. This test doesn’t care about that, which makes it a safe choice.

Results:
The p-value was 0.00001 (basically zero), so there’s definitely a difference in seed yield between the treatments.

Interpretation:
The treatments clearly matter for seed yield. The boxplot I made shows how the treatments vary, and you can tell some of them are better at boosting yield than others. If this were a real experiment, I’d probably run follow-up tests to figure out which treatments are the best.